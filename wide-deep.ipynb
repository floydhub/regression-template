{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# regression-template\n",
    "\n",
    "Hi 🙂, if you are seeing this notebook, you have succesfully started your first project on FloydHub 🚀, hooray!!\n",
    "Predicting the price of an object given the historical data is one of the most common task of [ML](https://en.wikipedia.org/wiki/Machine_learning), usually achieved with the [Linear Regression model](https://en.wikipedia.org/wiki/Linear_regression). In this project the Linear Layer will be only the top of the iceberg of a model which combines the wideness of ML model and the deepness of DL model for NLP. The goal is to predict the price of a wine from its description (and variety).\n",
    "\n",
    "### Predicting price of wine\n",
    "\n",
    "In this notebook, we will build a classifier to correctly predict the price of a wine from its description. More in detail, we will combine the strength of ML and DL learning using a [Wide & Deep Model](https://medium.com/tensorflow/predicting-the-price-of-wine-with-the-keras-functional-api-and-tensorflow-a95d1c2c1b03), which provides really good performance for Regression and Recommendation tasks.\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/floydhub/regression-template/master/images/wineprice.png\" width=\"800\" height=\"800\" align=\"center\"/>\n",
    "\n",
    "We will use the [Kaggle's Wine Reviews dataset](https://www.kaggle.com/zynicide/wine-reviews) for training our model. The dataset contains 10 columns and 150k rows of wine reviews.\n",
    "\n",
    "We will:\n",
    "\n",
    "- Preprocess text data for NLP\n",
    "- Build and train a [Wide & Deep model](https://ai.googleblog.com/2016/06/wide-deep-learning-better-together-with.html) using Keras and Tensorflow\n",
    "- Evaluate our model on the test set\n",
    "- Run the model on your own movie reviews!\n",
    "\n",
    "\n",
    "### Instructions\n",
    "- To execute a code cell, click on the cell and press `Shift + Enter` (shortcut for Run).\n",
    "- To learn more about Workspaces, check out the [Getting Started Notebook](get_started_workspace.ipynb).\n",
    "- **Tip**: *Feel free to try this Notebook with your own data and on your own super awesome regression task.*\n",
    "\n",
    "Now, let's get started! 🚀"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial Setup\n",
    "Let's start by importing the packages, setting the training variables and loading the csv file from which get all the data we need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "height": 321,
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2880,
     "status": "error",
     "timestamp": 1505781339378,
     "user": {
      "displayName": "Sara Robinson",
      "photoUrl": "//lh4.googleusercontent.com/-RR9n0dvbwgI/AAAAAAAAAAI/AAAAAAAAMYM/SOr5ZExpvXE/s50-c-k-no/photo.jpg",
      "userId": "112510032804989247452"
     },
     "user_tz": 240
    },
    "id": "783h64rGhA3T",
    "outputId": "d447b2ab-e321-4ee5-abd4-de2c0116302f"
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import itertools\n",
    "import os\n",
    "import math\n",
    "import string\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow import keras\n",
    "layers = keras.layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Parameters\n",
    "\n",
    "We'll set the hyperparameters for training our model. If you understand what they mean, feel free to play around - otherwise, we recommend keeping the defaults for your first run 🙂"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparams if GPU is available\n",
    "if tf.test.is_gpu_available():\n",
    "    # GPU\n",
    "    BATCH_SIZE = 256  # Number of examples used in each iteration\n",
    "    EPOCHS = 10  # Number of passes through entire dataset\n",
    "    MAX_LEN = 170  # Max length of review (in words)\n",
    "    VOCAB_SIZE = 1000  # Size of vocabulary dictionary\n",
    "    EMBEDDING = 8  # Dimension of word embedding vector\n",
    "    \n",
    "# Hyperparams for CPU training\n",
    "else:\n",
    "    # CPU\n",
    "    BATCH_SIZE = 128\n",
    "    EPOCHS = 10\n",
    "    MAX_LEN = 170\n",
    "    VOCAB_SIZE = 1000\n",
    "    EMBEDDING = 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data\n",
    "\n",
    "The wine reviews dataset is already attached to your workspace (if you want to attach your own data, [check out our docs](https://docs.floydhub.com/guides/workspace/#attaching-floydhub-datasets)).\n",
    "\n",
    "Let's take a look at data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>country</th>\n",
       "      <th>description</th>\n",
       "      <th>designation</th>\n",
       "      <th>points</th>\n",
       "      <th>price</th>\n",
       "      <th>province</th>\n",
       "      <th>region_1</th>\n",
       "      <th>region_2</th>\n",
       "      <th>variety</th>\n",
       "      <th>winery</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>147097</th>\n",
       "      <td>147098</td>\n",
       "      <td>US</td>\n",
       "      <td>Aged in American oak, with mostly similar vine...</td>\n",
       "      <td>C'est Beaux</td>\n",
       "      <td>88</td>\n",
       "      <td>35.0</td>\n",
       "      <td>Washington</td>\n",
       "      <td>Columbia Valley (WA)</td>\n",
       "      <td>Columbia Valley</td>\n",
       "      <td>Syrah</td>\n",
       "      <td>Walter Dacon</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44585</th>\n",
       "      <td>44585</td>\n",
       "      <td>US</td>\n",
       "      <td>A good, strong, sturdy wine, filled with class...</td>\n",
       "      <td>The Philanthropist Yountville Estate Vineyard</td>\n",
       "      <td>88</td>\n",
       "      <td>52.0</td>\n",
       "      <td>California</td>\n",
       "      <td>Napa Valley</td>\n",
       "      <td>Napa</td>\n",
       "      <td>Cabernet Sauvignon</td>\n",
       "      <td>Markham</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38555</th>\n",
       "      <td>38555</td>\n",
       "      <td>France</td>\n",
       "      <td>Dry tannins dominate this wine. Pavie's style ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>94</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Bordeaux</td>\n",
       "      <td>Saint-Émilion</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Bordeaux-style Red Blend</td>\n",
       "      <td>Château Pavie</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57566</th>\n",
       "      <td>57566</td>\n",
       "      <td>US</td>\n",
       "      <td>100% Syrah from the same mix of vineyards as t...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>90</td>\n",
       "      <td>24.0</td>\n",
       "      <td>Washington</td>\n",
       "      <td>Walla Walla Valley (WA)</td>\n",
       "      <td>Columbia Valley</td>\n",
       "      <td>Syrah</td>\n",
       "      <td>Watermill</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43906</th>\n",
       "      <td>43906</td>\n",
       "      <td>France</td>\n",
       "      <td>The nose displays some funky notes alongside p...</td>\n",
       "      <td>Les Princes Abbés</td>\n",
       "      <td>85</td>\n",
       "      <td>15.0</td>\n",
       "      <td>Alsace</td>\n",
       "      <td>Alsace</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Pinot Blanc</td>\n",
       "      <td>Domaines Schlumberger</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Unnamed: 0 country                                        description  \\\n",
       "147097      147098      US  Aged in American oak, with mostly similar vine...   \n",
       "44585        44585      US  A good, strong, sturdy wine, filled with class...   \n",
       "38555        38555  France  Dry tannins dominate this wine. Pavie's style ...   \n",
       "57566        57566      US  100% Syrah from the same mix of vineyards as t...   \n",
       "43906        43906  France  The nose displays some funky notes alongside p...   \n",
       "\n",
       "                                          designation  points  price  \\\n",
       "147097                                    C'est Beaux      88   35.0   \n",
       "44585   The Philanthropist Yountville Estate Vineyard      88   52.0   \n",
       "38555                                             NaN      94    NaN   \n",
       "57566                                             NaN      90   24.0   \n",
       "43906                               Les Princes Abbés      85   15.0   \n",
       "\n",
       "          province                 region_1         region_2  \\\n",
       "147097  Washington     Columbia Valley (WA)  Columbia Valley   \n",
       "44585   California              Napa Valley             Napa   \n",
       "38555     Bordeaux            Saint-Émilion              NaN   \n",
       "57566   Washington  Walla Walla Valley (WA)  Columbia Valley   \n",
       "43906       Alsace                   Alsace              NaN   \n",
       "\n",
       "                         variety                 winery  \n",
       "147097                     Syrah           Walter Dacon  \n",
       "44585         Cabernet Sauvignon                Markham  \n",
       "38555   Bordeaux-style Red Blend          Château Pavie  \n",
       "57566                      Syrah              Watermill  \n",
       "43906                Pinot Blanc  Domaines Schlumberger  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = '/floyd/input/winereviews/wine_data.csv'  # ADD path/to/dataset\n",
    "\n",
    "# Convert the data to a Pandas data frame\n",
    "data = pd.read_csv(path)\n",
    "\n",
    "# Shuffle the data\n",
    "data = data.sample(frac=1)\n",
    "\n",
    "# Print the first 5 rows\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are some data cleaning step:\n",
    "\n",
    "- Remove missing values\n",
    "- Get only Varieties which appear more frequently (>= 500 times). \n",
    "\n",
    "Then split the dataset: 80 (train) - 20 (test)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: 95646\n",
      "Test size: 23912\n"
     ]
    }
   ],
   "source": [
    "# Do some preprocessing to limit the # of wine varities in the dataset\n",
    "# Clean it from null values\n",
    "data = data[pd.notnull(data['country'])]\n",
    "data = data[pd.notnull(data['price'])]\n",
    "data = data.drop(data.columns[0], axis=1) \n",
    "\n",
    "variety_threshold = 500 # Anything that occurs less than this will be removed.\n",
    "value_counts = data['variety'].value_counts()\n",
    "to_remove = value_counts[value_counts <= variety_threshold].index\n",
    "data.replace(to_remove, np.nan, inplace=True)\n",
    "data = data[pd.notnull(data['variety'])]\n",
    "\n",
    "# Split data into train and test\n",
    "train_size = int(len(data) * .8)\n",
    "print (\"Train size: %d\" % train_size)\n",
    "print (\"Test size: %d\" % (len(data) - train_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Custom Tokenizer\n",
    "re_tok = re.compile(f'([{string.punctuation}“”¨«»®´·º½¾¿¡§£₤‘’])')\n",
    "def tokenize(s): return re_tok.sub(r' \\1 ', s).split()\n",
    "\n",
    "# Plot sentence by length\n",
    "plt.hist([len(tokenize(s)) for s in data['description'].values], bins=50)\n",
    "plt.title('Token per sentence')\n",
    "plt.xlabel('Len (number of token)')\n",
    "plt.ylabel('# samples')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The *Tokens per sentence* plot (see above) is useful for setting the `MAX_LEN` training hyperparameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train features\n",
    "description_train = data['description'][:train_size]\n",
    "variety_train = data['variety'][:train_size]\n",
    "\n",
    "# Train labels\n",
    "labels_train = data['price'][:train_size]\n",
    "\n",
    "# Test features\n",
    "description_test = data['description'][train_size:]\n",
    "variety_test = data['variety'][train_size:]\n",
    "\n",
    "# Test labels\n",
    "labels_test = data['price'][train_size:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wide Representation: BoW\n",
    "\n",
    "The Code below will encode the description of each sentence using the  [BoW model](https://en.wikipedia.org/wiki/Bag-of-words_model). This representation will encode each sentence to a vector that keeps track of the entries in the vocabulary which are used in the current sentences. This step will build a sparse vector(a vector with mostly zero values) for each description. The Code provides an example to help you get the intuition behind it.\n",
    "\n",
    "The **wide**  term used for defining this model is due to the sparse representation that this type of encoding carried out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "anD38iilhA3r"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Original Sample: Aged in American oak, with mostly similar vineyard sources. The American oak, with more spice to the fruit, and less smoothness overall than the C'est Belle Bottling. Cherry and berry flavors abound; it's sturdy, solid and well-made. Just 170 cases produced.\n",
      "\n",
      "First Sample after BoW (sparse representation truncated at the first 100 vocabulary terms): [0. 1. 1. 0. 0. 1. 0. 0. 1. 0. 1. 1. 0. 1. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "print(\"First Original Sample:\", data['description'].values[0])\n",
    "# Create a tokenizer to preprocess our text descriptions\n",
    "tokenizer = keras.preprocessing.text.Tokenizer(num_words=VOCAB_SIZE, char_level=False)\n",
    "tokenizer.fit_on_texts(description_train) # only fit on train\n",
    "\n",
    "# Wide feature 1: sparse bag of words (bow) vocab_size vector \n",
    "description_bow_train = tokenizer.texts_to_matrix(description_train)\n",
    "description_bow_test = tokenizer.texts_to_matrix(description_test)\n",
    "print(\"\\nFirst Sample after BoW (sparse representation truncated at the first 100 vocabulary terms):\", description_bow_train[0][:100])\n",
    "\n",
    "# Wide feature 2: one-hot vector of variety categories\n",
    "# Use sklearn utility to convert label strings to numbered index\n",
    "encoder = LabelEncoder()\n",
    "encoder.fit(variety_train)\n",
    "variety_train = encoder.transform(variety_train)\n",
    "variety_test = encoder.transform(variety_test)\n",
    "num_classes = np.max(variety_train) + 1\n",
    "\n",
    "# Convert labels to one hot\n",
    "variety_train = keras.utils.to_categorical(variety_train, num_classes)\n",
    "variety_test = keras.utils.to_categorical(variety_test, num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wide Model\n",
    "\n",
    "The model will use the BoW representation for the *wine description* and One-Hot encoding representation for the *wine variety* as Features for the Wide Model (a 2 layers NN).\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/floydhub/regression-template/master/images/wide.png\" width=\"350\" height=\"350\" align=\"center\"/>\n",
    "\n",
    "\n",
    "*Image from the [paper](https://arxiv.org/pdf/1606.07792.pdf)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 1000)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            (None, 40)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 1040)         0           input_1[0][0]                    \n",
      "                                                                 input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 256)          266496      concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 1)            257         dense_1[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 266,753\n",
      "Trainable params: 266,753\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Define our wide model with the functional API\n",
    "bow_inputs = layers.Input(shape=(VOCAB_SIZE,))\n",
    "variety_inputs = layers.Input(shape=(num_classes,))\n",
    "merged_layer = layers.concatenate([bow_inputs, variety_inputs])\n",
    "merged_layer = layers.Dense(256, activation='relu')(merged_layer)\n",
    "predictions = layers.Dense(1)(merged_layer)\n",
    "wide_model = keras.Model(inputs=[bow_inputs, variety_inputs], outputs=predictions)\n",
    "\n",
    "wide_model.compile(loss='mse', optimizer='adam', metrics=['accuracy'])\n",
    "print(wide_model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Representation: Embedding\n",
    "\n",
    "The Code below will encode the description of each sentence using [Word Embedding](https://en.wikipedia.org/wiki/Word_embedding). This representation will encode each word of the sentence into a vector. Before applying this encoding we need to preprocess the wine description by converting each token to an index and pad the sentence to the same length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Original Sample: Aged in American oak, with mostly similar vineyard sources. The American oak, with more spice to the fruit, and less smoothness overall than the C'est Belle Bottling. Cherry and berry flavors abound; it's sturdy, solid and well-made. Just 170 cases produced.\n",
      "\n",
      "First Sample after Preprocessing for Embedding: [362  10  33   5 789 135   2  33   5  59  35  11   2  13   1 509 312 122\n",
      "   2 349  19   1  44   8  17 911 183   1  55 124  89 495 523   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0]\n"
     ]
    }
   ],
   "source": [
    "print(\"First Original Sample:\", data['description'].values[0])\n",
    "\n",
    "# Deep model feature: word embeddings of wine descriptions\n",
    "train_embed = tokenizer.texts_to_sequences(description_train)\n",
    "test_embed = tokenizer.texts_to_sequences(description_test)\n",
    "\n",
    "train_embed = keras.preprocessing.sequence.pad_sequences(\n",
    "    train_embed, maxlen=MAX_LEN, padding=\"post\")\n",
    "test_embed = keras.preprocessing.sequence.pad_sequences(\n",
    "    test_embed, maxlen=MAX_LEN, padding=\"post\")\n",
    "\n",
    "print(\"\\nFirst Sample after Preprocessing for Embedding:\", train_embed[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Model\n",
    "\n",
    "This model build a liner layer at the top of the word embedding representaion of the wine description. \n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/floydhub/regression-template/master/images/deep.png\" width=\"450\" height=\"450\" align=\"center\"/>\n",
    "\n",
    "*Image from the [paper](https://arxiv.org/pdf/1606.07792.pdf)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_3 (InputLayer)         (None, 170)               0         \n",
      "_________________________________________________________________\n",
      "embedding_1 (Embedding)      (None, 170, 8)            8000      \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 1360)              0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1)                 1361      \n",
      "=================================================================\n",
      "Total params: 9,361\n",
      "Trainable params: 9,361\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Define our deep model with the Functional API\n",
    "deep_inputs = layers.Input(shape=(MAX_LEN,))\n",
    "embedding = layers.Embedding(VOCAB_SIZE, EMBEDDING, input_length=MAX_LEN)(deep_inputs)\n",
    "embedding = layers.Flatten()(embedding)\n",
    "embed_out = layers.Dense(1)(embedding)\n",
    "deep_model = keras.Model(inputs=deep_inputs, outputs=embed_out)\n",
    "print(deep_model.summary())\n",
    "\n",
    "deep_model.compile(loss='mse',\n",
    "                       optimizer='adam',\n",
    "                       metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wide & Deep Model\n",
    "\n",
    "We will implement a model similar to Heng-Tze Cheng’s [Wide & Deep Learning for Recommender Systems](https://arxiv.org/pdf/1606.07792.pdf). \n",
    "\n",
    "This model catenate the output of the previous models and build an additional linear layer at the top.\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/floydhub/regression-template/master/images/wide&deep.png\" width=\"500\" height=\"500\" align=\"center\"/>\n",
    "\n",
    "*Image from the [paper](https://arxiv.org/pdf/1606.07792.pdf)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 1000)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            (None, 40)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_3 (InputLayer)            (None, 170)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 1040)         0           input_1[0][0]                    \n",
      "                                                                 input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, 170, 8)       8000        input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 256)          266496      concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)             (None, 1360)         0           embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 1)            257         dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 1)            1361        flatten_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 2)            0           dense_2[0][0]                    \n",
      "                                                                 dense_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 1)            3           concatenate_2[0][0]              \n",
      "==================================================================================================\n",
      "Total params: 276,117\n",
      "Trainable params: 276,117\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Combine wide and deep into one model\n",
    "merged_out = layers.concatenate([wide_model.output, deep_model.output])\n",
    "merged_out = layers.Dense(1)(merged_out)\n",
    "combined_model = keras.Model(wide_model.input + [deep_model.input], merged_out)\n",
    "print(combined_model.summary())\n",
    "\n",
    "combined_model.compile(loss='mse',\n",
    "                       optimizer='adam',\n",
    "                       metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train& Eval\n",
    "\n",
    "If you left the default hyperpameters in the Notebook untouched, your training should take approximately:\n",
    "\n",
    "- On CPU machine: 1 minutes for 10 epochs.\n",
    "- On GPU machine: 30 seconds for 10 epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "gtP-hDRZhA31"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "95646/95646 [==============================] - 6s 66us/step - loss: 1135.1612 - acc: 0.0261\n",
      "Epoch 2/10\n",
      "95646/95646 [==============================] - 6s 61us/step - loss: 941.6884 - acc: 0.0326\n",
      "Epoch 3/10\n",
      "95646/95646 [==============================] - 6s 62us/step - loss: 880.5090 - acc: 0.0344\n",
      "Epoch 4/10\n",
      "95646/95646 [==============================] - 6s 61us/step - loss: 815.9317 - acc: 0.0345\n",
      "Epoch 5/10\n",
      "95646/95646 [==============================] - 6s 63us/step - loss: 742.0993 - acc: 0.0358\n",
      "Epoch 6/10\n",
      "95646/95646 [==============================] - 6s 64us/step - loss: 665.1590 - acc: 0.0362\n",
      "Epoch 7/10\n",
      "95646/95646 [==============================] - 6s 64us/step - loss: 588.6375 - acc: 0.0372\n",
      "Epoch 8/10\n",
      "95646/95646 [==============================] - 6s 64us/step - loss: 518.4199 - acc: 0.0388\n",
      "Epoch 9/10\n",
      "95646/95646 [==============================] - 6s 63us/step - loss: 448.8580 - acc: 0.0386\n",
      "Epoch 10/10\n",
      "95646/95646 [==============================] - 6s 63us/step - loss: 389.8381 - acc: 0.0401\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras._impl.keras.callbacks.History at 0x7f6afc721be0>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Run training\n",
    "combined_model.fit([description_bow_train, variety_train] + [train_embed], labels_train, epochs=EPOCHS, batch_size=BATCH_SIZE, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23912/23912 [==============================] - 1s 21us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1050.6474193012727, 0.03379056540711363]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_model.evaluate([description_bow_test, variety_test] + [test_embed], labels_test, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7377b410e34b41fa99d22c265d0acf51",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=1, description='# of test to evaluate/show', max=20, min=1), Output()), …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from ipywidgets import interact\n",
    "from ipywidgets import widgets\n",
    "\n",
    "def evaluate(num_predictions):\n",
    "    # Generate predictions\n",
    "    predictions = combined_model.predict([description_bow_test, variety_test] + [test_embed])\n",
    "\n",
    "    # Compare predictions with actual values for the first few items in our test dataset\n",
    "    diff = 0\n",
    "    for i in range(num_predictions):\n",
    "        val = predictions[i]\n",
    "        print('[{}] - {}'.format(i+1, description_test.iloc[i]))\n",
    "        print('Predicted: ', val[0], 'Actual: ', labels_test.iloc[i], '\\n')\n",
    "        diff += abs(val[0] - labels_test.iloc[i])\n",
    "\n",
    "    # Compare the average difference between actual price and the model's predicted price\n",
    "    print('Average prediction difference: ', diff / num_predictions)\n",
    "\n",
    "interact(evaluate, num_predictions=widgets.IntSlider(value=1, min=1, max=20, description='# of test to evaluate/show'));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## It's your turn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test out the model you just trained. Run the code Cell below and type your reviews in the widget, Have fun!🎉\n",
    "\n",
    "Here are some inspirations:\n",
    "\n",
    "- **Description**: 'From 18-year-old vines, this supple well-balanced effort blends flavors of mocha, cherry, vanilla and breakfast tea. Superbly integrated and delicious even at this early stage, this wine seems destined for a long and savory cellar life. Drink now through 2028.', **Variety**: 'Pinot Noir'.\n",
    "- **Description**: 'The Quarts de Chaume, the four fingers of land that rise above the Layon Valley, are one of the pinnacles of sweet wines in the Loire. Showing botrytis and layers of dryness over the honey and peach jelly flavors, but also has great freshness. The aftertaste just lasts.', **Variety**: 'Chenin Blanc'.\n",
    "- **Description**: 'Nicely oaked blackberry, licorice, vanilla and charred aromas are smooth and sultry. This is an outstanding wine from an excellent year. Forward barrel-spice and mocha flavors adorn core blackberry and raspberry fruit, while this runs long and tastes vaguely chocolaty on the velvety finish. Enjoy this top-notch Tempranillo through 2030.', **Variety**: 'Tempranillo'.\n",
    "- **Description**: 'Bright, light oak shadings dress up this medium-bodied wine, complementing the red cherry and strawberry flavors. Its fresh, fruity and not very tannic—easy to drink and enjoy.', **Variety**: 'Sauvignon Blanc'.\n",
    "- **Description**: 'This wine features black cherry, blackberry, blueberry with aromas of black licorice and earth. Ending with a creamy vanilla finish.', **Variety**: 'Syrah'.\n",
    "\n",
    "Can you do better? Play around with the model hyperparameters!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "abffcfd59e4b4e1c904e9c5c26d0d8a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(Textarea(value='', description='test_description', placeholder='Type a wine Description …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from ipywidgets import interact_manual\n",
    "from ipywidgets import widgets\n",
    "\n",
    "\n",
    "def get_prediction(test_description, test_variety):\n",
    "    # Wide model features\n",
    "    bow_description = tokenizer.texts_to_matrix([test_description])\n",
    "    variety = encoder.transform([test_variety])\n",
    "    variety = keras.utils.to_categorical(variety, len(encoder.classes_))\n",
    "    \n",
    "    # Deep model feature: word embeddings of wine descriptions\n",
    "    embed_description = tokenizer.texts_to_sequences([test_description])\n",
    "    embed_description = keras.preprocessing.sequence.pad_sequences(\n",
    "        embed_description, maxlen=MAX_LEN, padding=\"post\")\n",
    "    \n",
    "\n",
    "    # Evaluate\n",
    "    predictions = combined_model.predict([bow_description, variety] + [embed_description])\n",
    "    print('DESCRIPTION:', test_description)\n",
    "    print('VARIETY:', test_variety)\n",
    "    print('PREDICTED:', predictions[0][0])\n",
    "    \n",
    "interact_manual(get_prediction, \n",
    "                test_description=widgets.Textarea(placeholder='Type a wine Description here'),\n",
    "                test_variety=widgets.Text(placeholder='Type a wine Variety here'));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save your model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Saving Tokenizer\n",
    "with open('models/tokenizer.pickle', 'wb') as handle:\n",
    "    pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    " \n",
    "# Saving Variety Encode\n",
    "with open('models/encoder.pickle', 'wb') as handle:\n",
    "    pickle.dump(encoder, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    \n",
    "# Saving Model Weight\n",
    "combined_model.save_weights('models/wide_and_deep_weights.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### That's all folks - don't forget to shutdown your workspace once you're done 🙂"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "default_view": {},
   "name": "keras-wide-deep.ipynb",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
